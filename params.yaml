data_preparation:
  train_test_ratio: 0.8
  train_val_ratio: 0.9
  random_state: 42
linear_regression:
  model_name: 'LinearRegression'
decision_tree:
  DecisionTree:
    max_depth: [2, 10, 18, 26, 34, 42, 50, 58, 66, 74, 82, 90, 98]
    splitter: ['best', 'random']
    min_samples_split: [2 ,8, 14, 20, 26, 32, 38, 44]
    min_samples_leaf: [2 ,8, 14, 20, 26, 32, 38, 44]
  RandomForest:
    n_estimators: [2, 6, 10, 14, 20, 25, 40]
    max_depth: [2, 10, 18, 26, 34, 42, 50, 58, 66, 74]
    min_samples_split: [2 ,8, 14, 20]
    min_samples_leaf: [2 ,8, 14, 20]
  ExtraTree:
    n_estimators: [ 2, 6, 10, 14, 20, 25, 40 ]
    max_depth: [ 2, 10, 18, 26, 34, 42, 50, 58, 66, 74 ]
    min_samples_split: [ 2 ,8, 14, 20 ]
    min_samples_leaf: [ 2 ,8, 14, 20 ]
XGBoosting:
  model_name: 'XGBRegressor'
  n_estimators: [15, 50, 100]
  max_depth: [9, 15, 18]
  gamma: [1, 3, 5, 9]
  min_child_weight: [1, 3, 5, 9]

Cat:
  model_name: 'CatBoostRegressor'
  n_estimators: [50, 80]
  max_depth: [3, 4, 5, 9, 12]
Neyronich:
  n_of_neurons: [64, 128, 256]
  batch_size: [64, 96, 128]
  buffer_size: 256
  learning_rate: [0.1, 0.05, 0.01, 0.005]
  epochs: 200
Neyronich_prod:
  n_of_neurons: 128
  batch_size: 96
  learning_rate: 0.01
  epochs: 200
  buffer_size: 256
